{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acf6e5c6-6c1b-42c1-a834-2c4b65c913f0",
   "metadata": {},
   "source": [
    "### Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68e577-4403-4e6e-ad6c-cf78e4230be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#required modules\n",
    "!pip install .\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from cobra import io\n",
    "from jproperties import Properties\n",
    "from zipfile import ZipFile\n",
    "from mana import modelling, batchs, results_processing, dars\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cc4456d",
   "metadata": {},
   "source": [
    "### Load properties file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a665985",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = Properties()\n",
    "try:\n",
    "    with open('props.properties', 'rb') as config_file:\n",
    "        props.load(config_file)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    print(\"\\033[91m\\033[1m \"+\"You must provide a props.properties file\"+\" \\033[0m\\033[91m\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "261549f1-83fc-4cd9-9af0-95cb15c5e48e",
   "metadata": {},
   "source": [
    "### Load required datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be4743",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define paths \n",
    "### Load the model \n",
    "model = io.load_json_model(props.get(\"modelFile\").data)\n",
    "if len(props.get(\"modelId\").data) > 0:\n",
    "    model.id = str(props.get(\"modelId\").data)\n",
    "\n",
    "### Load the metadata file\n",
    "pheno = pd.read_csv(props.get(\"pheno\").data,sep=\"\\t\",index_col=0)\n",
    "\n",
    "### Get relevant data and process it\n",
    "#If cpds is empty, ask the user a list of compounds\n",
    "cpds = str(props.get(\"cpds\").data).split('/')\n",
    "if len(cpds) == 0:\n",
    "    input_buf = ''\n",
    "    while input_buf.lower() != 'stop':\n",
    "        input_buf = input('Provide either a molecule name or stop')\n",
    "        if input_buf.lower() != 'stop':\n",
    "            cpds.append(input_buf)\n",
    "\n",
    "#load barcode processed data\n",
    "gprs = modelling.get_GPR_reactions(model)\n",
    "hgnc_data = pd.read_csv(props.get(\"mappingFile\").data, sep=\"\\t\", dtype='unicode')\n",
    "data = pd.read_csv(props.get(\"data\").data,sep=\"\\t\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91a3bcd6",
   "metadata": {},
   "source": [
    "### Prepare datasets and working env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f30a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map gene expression identifiers and build sub dataset\n",
    "col_to_add = modelling.identify_model_gene_ids(model)\n",
    "if col_to_add == \"model not implemented\":\n",
    "    print(\"\\033[91m\\033[1m \"+\"Unknown model\"+\" \\033[0m\\033[91m\")\n",
    "data = modelling.map_single_column(data,hgnc_data,col_to_add)\n",
    "barcodes_to_keep = {}\n",
    "subset_data = pd.DataFrame()\n",
    "anot_cols = ['PROBEID','SYMBOL','HGNC ID','ENTREZID','GENENAME','N_GENES_IDENTICAL_PROBE']\n",
    "#build dir and go into the relevant dir\n",
    "dirname = str(props.get(\"working_path\").data)+str(props.get(\"dose\").data).lower()+'_'+str(props.get(\"time\").data).replace(' ','_')\n",
    "if dirname.split(\"/\")[-1] not in os.listdir(str(props.get(\"working_path\").data)):\n",
    "    os.mkdir(dirname)\n",
    "# os.chdir(dirname)\n",
    "working_folders = ['batch_renum/batch','csvs','full_rxn_enum_set','log_dir','working_renum']\n",
    "for name in working_folders:\n",
    "    if name.split('/')[0] not in os.listdir(dirname):\n",
    "        if name == \"batch_renum/batch\":\n",
    "            os.mkdir(dirname+\"/\"+name.split('/')[0])\n",
    "            os.mkdir(dirname+\"/\"+name)\n",
    "        else:\n",
    "            os.mkdir(dirname+\"/\"+name)\n",
    "            \n",
    "for cpd in str(props.get(\"cpds\").data).split(\"/\"):\n",
    "    barcodes_to_keep[cpd] = list(pheno[(pheno['dose_level'] == props.get(\"dose\").data) & \\\n",
    "                                       (pheno['sacri_period'] == props.get(\"time\").data) & (pheno['compound_name'] == cpd)].index)\n",
    "    if subset_data.shape[0] == 0:\n",
    "        subset_data = pd.DataFrame(data.loc[:,anot_cols+barcodes_to_keep[cpd]])\n",
    "    else:\n",
    "        subset_data = pd.concat([subset_data,pd.DataFrame(data.loc[:,barcodes_to_keep[cpd]])],axis=1)\n",
    "#extract data and binarize with the 75/25 method\n",
    "modelling.preprocess_data(subset_data,col_to_add,model,csvs_path=dirname+\"/csvs/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be6240e9",
   "metadata": {},
   "source": [
    "### Write reaction enum batch scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8076fd-b47c-468d-92a1-2a9d3892d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(dirname+\"/\"+'csvs/'):\n",
    "    if \".CEL\" in file:\n",
    "        batchs.write_rxn_enum_script(script_path=str(props.get(\"rxn_enum_script_path\").data),batch_directory=dirname+\"/\"+'batch_renum/',\\\n",
    "                                     output_directory=dirname+\"/\"+'working_renum',modelfile=props.get(\"modelFile\").data,\\\n",
    "                                     weightfile=dirname+\"/\"+'csvs/'+file,reactionFile=props.get(\"working_path\").data+\"input_data/recon2_2_reactions.csv\", para_batchs=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36fbc661-e4d2-4d58-bf25-d21e465e12ec",
   "metadata": {},
   "source": [
    "### Launch all the batchs on the cluster\n",
    "To run reaction enum on the cluster, you will need the dexom-python package with its dependencies installed (preferrably in a conda environment ) and CPLEX installed (to install locally on the genotoul cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc0920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing purpose only, get modelisation results and unzip them\n",
    "for file in glob.glob(props.get(\"working_path\").data+\"input_data/test_data/\"+\\\n",
    "                       str(props.get(\"dose\").data).lower()+\"_\"+\\\n",
    "                        str(props.get(\"time\").data).lower().replace(\" \",\"\")+\"_renum_sols.zip\"):\n",
    "    with ZipFile(file, 'r') as zObject:\n",
    "        zObject.extractall(\n",
    "            path=props.get(\"working_path\").data+\\\n",
    "                    str(props.get(\"dose\").data).lower()+\"_\"+\\\n",
    "                        str(props.get(\"time\").data).lower().replace(\" \",\"_\")+\\\n",
    "                            \"/working_renum/\")\n",
    "    print(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a73ac9f1-e590-4579-9d32-2d326b3fcf4d",
   "metadata": {},
   "source": [
    "### Check that all the batchs are done. If necessary launch the launch_failed_batch_reaction_enum.sh file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629941dd-cb38-400f-a4b0-1f2c200fffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_processing.remove_done_batchs(dirname+\"/\"+'batch_renum/batch/',dirname+\"/\"+'working_renum/',relax_param=True)\n",
    "if len(glob.glob(dirname+\"/\"+'batch_renum/batch/*.CEL*.sh',recursive=False))>0:\n",
    "    print(len(glob.glob(dirname+\"/\"+'batch_renum/batch/*.CEL*.sh',recursive=False)),' batchs to relaunch')\n",
    "    raise FileExistsError\n",
    "else:\n",
    "    print(\"All batchs have been processed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79a11f20-05f2-441c-8f3c-de4c5f3e0666",
   "metadata": {},
   "source": [
    "### Concatenate solutions in one csv file per biological condition and replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28f12f-a56b-4d5b-8735-cf4e62fe51b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_processing.concatenate_solutions(dirname+\"/\"+\"working_renum/\",dirname+\"/\"+\"full_rxn_enum_set\",col_index=\"\",ncpus=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "415abe97-4f55-4c7c-9ea9-80b6a43949d3",
   "metadata": {},
   "source": [
    "### Generate batch for diversity enum, starting from full reaction enum results\n",
    "\n",
    "To launch the diversity enum pipeline, you must have finished the following steps:\n",
    "* Full enumeration completed\n",
    "* Proabilities computation on all the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec4319-86fd-4237-84a3-396a6a7d09f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folders = ['batch_dexom/batch','prev_sol_dir','full_div_enum_set','working_divers']\n",
    "for name in working_folders:\n",
    "    if name.split('/')[0] not in os.listdir(dirname):\n",
    "        if name == \"batch_dexom/batch\":\n",
    "            os.mkdir(dirname+\"/\"+name.split('/')[0])\n",
    "            os.mkdir(dirname+\"/\"+name)\n",
    "        else:\n",
    "            os.mkdir(dirname+\"/\"+name)\n",
    "for file in os.listdir(dirname+\"/\"+'csvs/'):\n",
    "    batchs.write_div_enum_script(script_path=str(props.get(\"div_enum_script_path\").data),batch_directory = dirname+\"/\"+'batch_dexom',\n",
    "                                 rxn_enum_set_dir = dirname+\"/\"+'full_rxn_enum_set',output_directory = dirname+\"/\"+'working_divers',\n",
    "                                   modelfile = '../input_data/recon2v2_biomass_corrected.json',weightfile = dirname+\"/\"+'csvs/'+file,\n",
    "                                     reactionFile = props.get(\"working_path\").data+'input_data/recon2_2_reactions.csv',\n",
    "                                     prev_sol_dir = dirname+\"/\"+'prev_sol_dir/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a825d2b0-d6a4-4253-b1e6-ac6356b9f75b",
   "metadata": {},
   "source": [
    "### Launch all the batchs on the cluster\n",
    "To run reaction enum on the cluster, you will need the dexom-python package with its dependencies installed (preferrably in a conda environment ) and CPLEX installed (to install locally on the genotoul cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b91e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing purpose only, get modelisation results and unzip them\n",
    "for file in glob.glob(props.get(\"working_path\").data+\"input_data/test_data/\"+\\\n",
    "                       str(props.get(\"dose\").data).lower()+\"_\"+\\\n",
    "                        str(props.get(\"time\").data).lower().replace(\" \",\"\")+\"_divers_sols.zip\"):\n",
    "        with ZipFile(file, 'r') as zObject:\n",
    "              zObject.extractall(\n",
    "                     path=props.get(\"working_path\").data+\\\n",
    "                        str(props.get(\"dose\").data).lower()+\"_\"+\\\n",
    "                            str(props.get(\"time\").data).lower().replace(\" \",\"_\")+\\\n",
    "                                \"/working_divers/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27ca3c69-d462-474d-b2f5-f8b94e3bdbfb",
   "metadata": {},
   "source": [
    "### Check that all the batchs are done. If necessary launch the launch_failed_batch_reaction_enum.sh file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee93273-93ee-4a22-81ab-6262c93ae297",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_processing.remove_done_batchs(dirname+\"/\"+'batch_dexom/batch/',dirname+\"/\"+'working_divers/',relax_param=False)\n",
    "if len(glob.glob(dirname+\"/\"+'batch_dexom/batch/*.CEL*.sh',recursive=False))>0:\n",
    "    print(len(glob.glob(dirname+\"/\"+'batch_dexom/batch/*.CEL*.sh',recursive=False)),' batchs to relaunch')\n",
    "    raise FileExistsError\n",
    "else:\n",
    "    print(\"All batchs have been processed\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b96ed2d-aa96-4732-bdaa-ce467f360e4b",
   "metadata": {},
   "source": [
    "### Concatenate solutions in one csv file per biological condition and replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae7d35-5976-46cd-9a57-e8077ff470c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_processing.concatenate_solutions(dirname+\"/\"+\"working_divers/\",dirname+\"/\"+\"full_div_enum_set\",col_index=\"\",ncpus=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50e6005c-836c-4a7d-9b38-2eee93840617",
   "metadata": {},
   "source": [
    "### Remove zero biomass solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c725ee80-0b7b-4acf-bb44-866b459fba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_processing.remove_zerobiomass_solutions(dirname+\"/\"+'full_rxn_enum_set',props.get(\"working_path\").data+'input_data/recon2_2_reactions.csv')\n",
    "results_processing.remove_zerobiomass_solutions(dirname+\"/\"+'full_div_enum_set',props.get(\"working_path\").data+'input_data/recon2_2_reactions.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "675788fe-a86f-4629-af38-8c5072c27726",
   "metadata": {},
   "source": [
    "### Concatenate the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc44efe-30dc-486c-8b04-bc0f0caba6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"full_enum\" not in os.listdir(dirname+\"/\"):\n",
    "    os.mkdir(dirname+\"/\"+\"full_enum\")\n",
    "results_processing.concatenate_reaction_div_enum(path_concat_rxn_enum = dirname+\"/\"+'full_rxn_enum_set',\n",
    "                                                 path_concat_div_enum = dirname+\"/\"+'full_div_enum_set',\n",
    "                                                  out_dir = dirname+\"/\"+\"full_enum/\",col_index=\"\",single_csv=False,ncpus=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
